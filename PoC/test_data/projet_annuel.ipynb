{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783ffcf4",
   "metadata": {},
   "source": [
    "## lecture PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e45df60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ce document décrit le syllabus d'un projet de Deep Learning intitulé \"Kaggle Dataset Analysis with Keras\" pour les étudiants en S1.  \n",
      "\n",
      "Le projet vise à familiariser les étudiants avec le framework keras_core (ou TensorFlow) en leur permettant de construire et d'évaluer différents modèles de Deep Learning sur des jeux de données du site Kaggle.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.retrievers import InMemoryBM25Retriever\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "\n",
    "# Configurez vos clés d'API ici\n",
    "os.environ[\"HF_TOKEN_API\"] = \"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\"\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "class Indexing:\n",
    "    def __init__(self, document_store):\n",
    "        self.converter = PyPDFToDocument()\n",
    "        self.cleaner = DocumentCleaner()\n",
    "        self.splitter = DocumentSplitter(split_by=\"sentence\", split_length=10, split_overlap=2)\n",
    "        self.writer = DocumentWriter(document_store=document_store)\n",
    "        self.pipeline = Pipeline()\n",
    "        self.pipeline.add_component(\"converter\", self.converter)\n",
    "        self.pipeline.add_component(\"cleaner\", self.cleaner)\n",
    "        self.pipeline.add_component(\"splitter\", self.splitter)\n",
    "        self.pipeline.add_component(\"writer\", self.writer)\n",
    "\n",
    "        self.pipeline.connect(\"converter\", \"cleaner\")\n",
    "        self.pipeline.connect(\"cleaner\", \"splitter\")\n",
    "        self.pipeline.connect(\"splitter\", \"writer\")\n",
    "\n",
    "    def get_pipeline(self) -> Pipeline:\n",
    "        return self.pipeline\n",
    "\n",
    "    def run_index_pipeline(self, path):\n",
    "        self.pipeline.run({\"converter\": {\"sources\": [Path(path)]}})\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, document_store, generator):\n",
    "        self.template = \"\"\"\n",
    "                        {% for document in documents %}\n",
    "                            {{ document.content }}\n",
    "                        {% endfor %}\n",
    "\n",
    "                        Please answer the question based on the given information.\n",
    "\n",
    "                        {{question}}\n",
    "                        \"\"\"\n",
    "        self.prompt_builder = PromptBuilder(template=self.template)\n",
    "        self.rag_pipeline = Pipeline()\n",
    "        self.retriever = InMemoryBM25Retriever(document_store)\n",
    "\n",
    "        self.rag_pipeline.add_component(\"retriever\", self.retriever)\n",
    "        self.rag_pipeline.add_component(\"prompt_builder\", self.prompt_builder)\n",
    "        self.rag_pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "        self.rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "        self.rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "    def run_pipeline(self, query):\n",
    "        res = self.rag_pipeline.run(\n",
    "            {\n",
    "                \"retriever\": {\"query\": query},\n",
    "                \"prompt_builder\": {\"question\": query},\n",
    "            }\n",
    "        )\n",
    "        return res\n",
    "\n",
    "# Initialisation du document store et des composants\n",
    "doc_store = InMemoryDocumentStore()\n",
    "idx = Indexing(doc_store)\n",
    "\n",
    "# Spécifiez ici le chemin du fichier PDF que vous voulez indexer\n",
    "pdf_path = \"C:/Users/sarra/Downloads/SyllabusDuProjet.pdf\"  # Remplacez par le chemin réel du fichier\n",
    "idx.run_index_pipeline(pdf_path)\n",
    "\n",
    "# Initialisation du générateur\n",
    "generator = OpenAIGenerator(\n",
    "    api_key=Secret.from_env_var(\"GROQ_API_KEY\"),\n",
    "    api_base_url=\"https://api.groq.com/openai/v1\",\n",
    "    model=\"gemma2-9b-it\",\n",
    "    generation_kwargs={\"max_tokens\": 4096},\n",
    ")\n",
    "\n",
    "# Création de l'objet de requête\n",
    "query = Query(doc_store, generator)\n",
    "\n",
    "# Spécifiez ici la requête que vous souhaitez exécuter\n",
    "question = \"quel est le sujet de ce document?\"  # Remplacez par votre question\n",
    "\n",
    "# Exécution de la requête et affichage de la réponse\n",
    "response = query.run_pipeline(question)\n",
    "print(response[\"llm\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd57f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30a19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf9d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d866b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f478c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4fd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5ad75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2380de6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d082c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e3c2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ba7dd91",
   "metadata": {},
   "source": [
    "## lecture PDF+TXT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639494ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarra\\anaconda3\\Lib\\site-packages\\haystack\\core\\errors.py:34: DeprecationWarning: PipelineMaxLoops is deprecated and will be remove in version '2.7.0'; use PipelineMaxComponentRuns instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document outlines the code for a genetic algorithm to evolve neural network architectures. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Data Preprocessing:**\n",
      "    * Loads CSV data and separates features (X) from target variable (y).\n",
      "    * Encodes categorical target variables.\n",
      "    * Splits data into training and testing sets.\n",
      "    * Scales features using StandardScaler.\n",
      "\n",
      "* **Genetic Algorithm Setup:**\n",
      "    * Defines a custom toolbox using DEAP library for genetic operations.\n",
      "    * Registers functions for individual creation (`generate_individual`), population creation (`generate_diverse_population`), attribute initialization (`attr_int`), and fitness evaluation (`eval_nn`).\n",
      "\n",
      "* **Neural Network Definition:**\n",
      "    *  `eval_nn` function builds and trains a simple sequential neural network based on the decoded individual's structure and weights.\n",
      "    *  Fitness is evaluated based on the model's performance (loss and accuracy) on the test set.\n",
      "\n",
      "* **Individual Representation:**\n",
      "    *  Individuals are represented as lists. The first part represents the number of layers and neurons in each layer, and the second part represents the weights.\n",
      "    *  Decoding (`decode_individual`) converts the individual representation into a structure for the neural network.\n",
      "\n",
      "* **Genetic Operators:**\n",
      "    *  Crossover (`mate`) combines parts of individuals to create offspring.\n",
      "    *  Mutation (`mutate_individual`) randomly changes parts of the individual.\n",
      "\n",
      "* **Algorithm Execution:**\n",
      "    *  The `run_genetic_algorithm` function runs the genetic algorithm for a specified number of generations, using the defined toolbox and fitness function.\n",
      "\n",
      "\n",
      "In summary, this code demonstrates a genetic algorithm approach to automatically find optimal neural network architectures for a given dataset. The algorithm iteratively improves the network architectures by evaluating their performance and selecting the best-performing ones for reproduction and mutation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument, TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.retrievers import InMemoryBM25Retriever\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "\n",
    "# Configurez vos clés d'API ici\n",
    "os.environ[\"HF_TOKEN_API\"] = \"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\"\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "class Indexing:\n",
    "    def __init__(self, document_store):\n",
    "        self.document_store = document_store\n",
    "\n",
    "    def create_pdf_pipeline(self):\n",
    "        \"\"\"Pipeline for indexing PDF files\"\"\"\n",
    "        pdf_pipeline = Pipeline()\n",
    "        pdf_converter = PyPDFToDocument()\n",
    "        cleaner = DocumentCleaner()\n",
    "        splitter = DocumentSplitter(split_by=\"sentence\", split_length=10, split_overlap=2)\n",
    "        writer = DocumentWriter(document_store=self.document_store)\n",
    "\n",
    "        pdf_pipeline.add_component(\"pdf_converter\", pdf_converter)\n",
    "        pdf_pipeline.add_component(\"cleaner\", cleaner)\n",
    "        pdf_pipeline.add_component(\"splitter\", splitter)\n",
    "        pdf_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "        pdf_pipeline.connect(\"pdf_converter\", \"cleaner\")\n",
    "        pdf_pipeline.connect(\"cleaner\", \"splitter\")\n",
    "        pdf_pipeline.connect(\"splitter\", \"writer\")\n",
    "        return pdf_pipeline\n",
    "\n",
    "    def create_txt_pipeline(self):\n",
    "        \"\"\"Pipeline for indexing TXT files\"\"\"\n",
    "        txt_pipeline = Pipeline()\n",
    "        text_converter = TextFileToDocument()\n",
    "        cleaner = DocumentCleaner()\n",
    "        splitter = DocumentSplitter(split_by=\"sentence\", split_length=10, split_overlap=2)\n",
    "        writer = DocumentWriter(document_store=self.document_store)\n",
    "\n",
    "        txt_pipeline.add_component(\"text_converter\", text_converter)\n",
    "        txt_pipeline.add_component(\"cleaner\", cleaner)\n",
    "        txt_pipeline.add_component(\"splitter\", splitter)\n",
    "        txt_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "        txt_pipeline.connect(\"text_converter\", \"cleaner\")\n",
    "        txt_pipeline.connect(\"cleaner\", \"splitter\")\n",
    "        txt_pipeline.connect(\"splitter\", \"writer\")\n",
    "        return txt_pipeline\n",
    "\n",
    "    def run_index_pipeline(self, path):\n",
    "        file_extension = Path(path).suffix.lower()\n",
    "        if file_extension == \".pdf\":\n",
    "            pdf_pipeline = self.create_pdf_pipeline()\n",
    "            pdf_pipeline.run({\"pdf_converter\": {\"sources\": [Path(path)]}})\n",
    "        elif file_extension == \".txt\":\n",
    "            txt_pipeline = self.create_txt_pipeline()\n",
    "            txt_pipeline.run({\"text_converter\": {\"sources\": [Path(path)]}})\n",
    "        else:\n",
    "            raise ValueError(\"Type de fichier non pris en charge. Veuillez utiliser un fichier PDF ou TXT.\")\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, document_store, generator):\n",
    "        self.template = \"\"\"\n",
    "                        {% for document in documents %}\n",
    "                            {{ document.content }}\n",
    "                        {% endfor %}\n",
    "\n",
    "                        Please answer the question based on the given information.\n",
    "\n",
    "                        {{question}}\n",
    "                        \"\"\"\n",
    "        self.prompt_builder = PromptBuilder(template=self.template)\n",
    "        self.rag_pipeline = Pipeline()\n",
    "        self.retriever = InMemoryBM25Retriever(document_store)\n",
    "\n",
    "        self.rag_pipeline.add_component(\"retriever\", self.retriever)\n",
    "        self.rag_pipeline.add_component(\"prompt_builder\", self.prompt_builder)\n",
    "        self.rag_pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "        self.rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "        self.rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "    def run_pipeline(self, query):\n",
    "        res = self.rag_pipeline.run(\n",
    "            {\n",
    "                \"retriever\": {\"query\": query},\n",
    "                \"prompt_builder\": {\"question\": query},\n",
    "            }\n",
    "        )\n",
    "        return res\n",
    "\n",
    "# Initialisation du document store et des composants\n",
    "doc_store = InMemoryDocumentStore()\n",
    "idx = Indexing(doc_store)\n",
    "\n",
    "# Spécifiez ici le chemin du fichier que vous voulez indexer\n",
    "file_path = \"C:/Users/sarra/Desktop/hachathon.txt\"  # Remplacez par le chemin réel du fichier\n",
    "#file_path = \"C:/Users/sarra/Downloads/SyllabusDuProjet.pdf\" \n",
    "idx.run_index_pipeline(file_path)\n",
    "\n",
    "# Initialisation du générateur\n",
    "generator = OpenAIGenerator(\n",
    "    api_key=Secret.from_env_var(\"GROQ_API_KEY\"),\n",
    "    api_base_url=\"https://api.groq.com/openai/v1\",\n",
    "    model=\"gemma2-9b-it\",\n",
    "    generation_kwargs={\"max_tokens\": 4096},\n",
    ")\n",
    "\n",
    "# Création de l'objet de requête\n",
    "query = Query(doc_store, generator)\n",
    "\n",
    "# Spécifiez ici la requête que vous souhaitez exécuter\n",
    "question = \"quel est le resumé de ce document?\"  # Remplacez par votre question\n",
    "\n",
    "# Exécution de la requête et affichage de la réponse\n",
    "response = query.run_pipeline(question)\n",
    "print(response[\"llm\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c445f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56056946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa1cea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c24742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4c6c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f157d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed10ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a732348",
   "metadata": {},
   "source": [
    "## lecture PDF+TXT+CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7cffa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide me with the document so I can summarize it for you. 😊  \n",
      "\n",
      "I need the text of the document to be able to understand it and create a summary. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument, TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.retrievers import InMemoryBM25Retriever\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "import pandas as pd\n",
    "from haystack import Document  # Ajout de l'import pour le type Document\n",
    "\n",
    "# Configurez vos clés d'API ici\n",
    "os.environ[\"HF_TOKEN_API\"] = \"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\"\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "class Indexing:\n",
    "    def __init__(self, document_store):\n",
    "        self.document_store = document_store\n",
    "\n",
    "    def create_pdf_pipeline(self):\n",
    "        \"\"\"Pipeline pour indexer les fichiers PDF\"\"\"\n",
    "        pdf_pipeline = Pipeline()\n",
    "        pdf_converter = PyPDFToDocument()\n",
    "        cleaner = DocumentCleaner()\n",
    "        splitter = DocumentSplitter(split_by=\"sentence\", split_length=10, split_overlap=2)\n",
    "        writer = DocumentWriter(document_store=self.document_store)\n",
    "\n",
    "        pdf_pipeline.add_component(\"pdf_converter\", pdf_converter)\n",
    "        pdf_pipeline.add_component(\"cleaner\", cleaner)\n",
    "        pdf_pipeline.add_component(\"splitter\", splitter)\n",
    "        pdf_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "        pdf_pipeline.connect(\"pdf_converter\", \"cleaner\")\n",
    "        pdf_pipeline.connect(\"cleaner\", \"splitter\")\n",
    "        pdf_pipeline.connect(\"splitter\", \"writer\")\n",
    "        return pdf_pipeline\n",
    "\n",
    "    def create_txt_pipeline(self):\n",
    "        \"\"\"Pipeline pour indexer les fichiers TXT\"\"\"\n",
    "        txt_pipeline = Pipeline()\n",
    "        text_converter = TextFileToDocument()\n",
    "        cleaner = DocumentCleaner()\n",
    "        splitter = DocumentSplitter(split_by=\"sentence\", split_length=10, split_overlap=2)\n",
    "        writer = DocumentWriter(document_store=self.document_store)\n",
    "\n",
    "        txt_pipeline.add_component(\"text_converter\", text_converter)\n",
    "        txt_pipeline.add_component(\"cleaner\", cleaner)\n",
    "        txt_pipeline.add_component(\"splitter\", splitter)\n",
    "        txt_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "        txt_pipeline.connect(\"text_converter\", \"cleaner\")\n",
    "        txt_pipeline.connect(\"cleaner\", \"splitter\")\n",
    "        txt_pipeline.connect(\"splitter\", \"writer\")\n",
    "        return txt_pipeline\n",
    "\n",
    "    def create_csv_pipeline(self, path):\n",
    "        \"\"\"Pipeline pour indexer les fichiers CSV\"\"\"\n",
    "        # Lecture du CSV\n",
    "        data = pd.read_csv(path)\n",
    "\n",
    "        # Création d'une liste de documents Haystack\n",
    "        documents = []\n",
    "        for _, row in data.iterrows():\n",
    "            content = ' '.join(f\"{col}: {val}\" for col, val in row.items())  # Conversion d'une ligne en texte\n",
    "            documents.append(Document(content=content))\n",
    "\n",
    "        # Ajout des documents au Document Store avec gestion des doublons\n",
    "        self.document_store.write_documents(documents, policy=\"SKIP\")  # Ignorer les doublons\n",
    "\n",
    "    def run_index_pipeline(self, path):\n",
    "        file_extension = Path(path).suffix.lower()\n",
    "        if file_extension == \".pdf\":\n",
    "            pdf_pipeline = self.create_pdf_pipeline()\n",
    "            pdf_pipeline.run({\"pdf_converter\": {\"sources\": [Path(path)]}})\n",
    "        elif file_extension == \".txt\":\n",
    "            txt_pipeline = self.create_txt_pipeline()\n",
    "            txt_pipeline.run({\"text_converter\": {\"sources\": [Path(path)]}})\n",
    "        elif file_extension == \".csv\":\n",
    "            self.create_csv_pipeline(path)\n",
    "        else:\n",
    "            raise ValueError(\"Type de fichier non pris en charge. Veuillez utiliser un fichier PDF, TXT ou CSV.\")\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, document_store, generator):\n",
    "        self.template = \"\"\"\n",
    "                        {% for document in documents %}\n",
    "                            {{ document.content }}\n",
    "                        {% endfor %}\n",
    "\n",
    "                        Please answer the question based on the given information.\n",
    "\n",
    "                        {{question}}\n",
    "                        \"\"\"\n",
    "        self.prompt_builder = PromptBuilder(template=self.template)\n",
    "        self.rag_pipeline = Pipeline()\n",
    "        self.retriever = InMemoryBM25Retriever(document_store)\n",
    "\n",
    "        self.rag_pipeline.add_component(\"retriever\", self.retriever)\n",
    "        self.rag_pipeline.add_component(\"prompt_builder\", self.prompt_builder)\n",
    "        self.rag_pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "        self.rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "        self.rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "    def run_pipeline(self, query):\n",
    "        res = self.rag_pipeline.run(\n",
    "            {\n",
    "                \"retriever\": {\"query\": query},\n",
    "                \"prompt_builder\": {\"question\": query},\n",
    "            }\n",
    "        )\n",
    "        return res\n",
    "\n",
    "# Initialisation du document store et des composants\n",
    "doc_store = InMemoryDocumentStore()\n",
    "idx = Indexing(doc_store)\n",
    "\n",
    "# Spécifiez ici le chemin du fichier que vous voulez indexer\n",
    "file_path = \"C:/Users/sarra/Desktop/iris.csv\"  # Remplacez par le chemin réel du fichier CSV\n",
    "idx.run_index_pipeline(file_path)\n",
    "\n",
    "# Initialisation du générateur\n",
    "generator = OpenAIGenerator(\n",
    "    api_key=Secret.from_env_var(\"GROQ_API_KEY\"),\n",
    "    api_base_url=\"https://api.groq.com/openai/v1\",\n",
    "    model=\"gemma2-9b-it\",\n",
    "    generation_kwargs={\"max_tokens\": 4096},\n",
    ")\n",
    "\n",
    "# Création de l'objet de requête\n",
    "query = Query(doc_store, generator)\n",
    "\n",
    "# Spécifiez ici la requête que vous souhaitez exécuter\n",
    "question = \"quel est le resumé de ce document?\"  # Remplacez par votre question\n",
    "\n",
    "# Exécution de la requête et affichage de la réponse\n",
    "response = query.run_pipeline(question)\n",
    "print(response[\"llm\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e78c738",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InMemoryDocumentStore' object has no attribute 'get_all_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 136\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Spécifiez ici le chemin du fichier que vous voulez indexer\u001b[39;00m\n\u001b[0;32m    135\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/sarra/Desktop/iris.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Remplacez par le chemin réel du fichier CSV\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m idx\u001b[38;5;241m.\u001b[39mrun_index_pipeline(file_path)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Initialisation du générateur\u001b[39;00m\n\u001b[0;32m    139\u001b[0m generator \u001b[38;5;241m=\u001b[39m OpenAIGenerator(\n\u001b[0;32m    140\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mSecret\u001b[38;5;241m.\u001b[39mfrom_env_var(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGROQ_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    141\u001b[0m     api_base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.groq.com/openai/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    142\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemma2-9b-it\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    143\u001b[0m     generation_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4096\u001b[39m},\n\u001b[0;32m    144\u001b[0m )\n",
      "Cell \u001b[1;32mIn[6], line 87\u001b[0m, in \u001b[0;36mIndexing.run_index_pipeline\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType de fichier non pris en charge. Veuillez utiliser un fichier PDF, TXT ou CSV.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Vérification : Afficher les documents dans le document_store\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m all_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_store\u001b[38;5;241m.\u001b[39mget_all_documents()\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocuments ajoutés au document_store : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m all_docs[:\u001b[38;5;241m5\u001b[39m]:  \u001b[38;5;66;03m# Afficher les 5 premiers documents pour vérification\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'InMemoryDocumentStore' object has no attribute 'get_all_documents'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from haystack import Pipeline\n",
    "from haystack.components.converters import PyPDFToDocument, TextFileToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner, DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.retrievers import InMemoryBM25Retriever\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.utils import Secret\n",
    "import os\n",
    "import pandas as pd\n",
    "from haystack import Document\n",
    "\n",
    "# Configurez vos clés d'API ici\n",
    "os.environ[\"HF_TOKEN_API\"] = \"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\"\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "class Indexing:\n",
    "    def __init__(self, document_store):\n",
    "        self.document_store = document_store\n",
    "\n",
    "    def create_pdf_pipeline(self):\n",
    "        \"\"\"Pipeline pour indexer les fichiers PDF\"\"\"\n",
    "        pdf_pipeline = Pipeline()\n",
    "        pdf_converter = PyPDFToDocument()\n",
    "        cleaner = DocumentCleaner()\n",
    "        splitter = DocumentSplitter(split_by=\"sentence\", split_length=10, split_overlap=2)\n",
    "        writer = DocumentWriter(document_store=self.document_store)\n",
    "\n",
    "        pdf_pipeline.add_component(\"pdf_converter\", pdf_converter)\n",
    "        pdf_pipeline.add_component(\"cleaner\", cleaner)\n",
    "        pdf_pipeline.add_component(\"splitter\", splitter)\n",
    "        pdf_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "        pdf_pipeline.connect(\"pdf_converter\", \"cleaner\")\n",
    "        pdf_pipeline.connect(\"cleaner\", \"splitter\")\n",
    "        pdf_pipeline.connect(\"splitter\", \"writer\")\n",
    "        return pdf_pipeline\n",
    "\n",
    "    def create_txt_pipeline(self):\n",
    "        \"\"\"Pipeline pour indexer les fichiers TXT\"\"\"\n",
    "        txt_pipeline = Pipeline()\n",
    "        text_converter = TextFileToDocument()\n",
    "        cleaner = DocumentCleaner()\n",
    "        splitter = DocumentSplitter(split_by=\"sentence\", split_length=10, split_overlap=2)\n",
    "        writer = DocumentWriter(document_store=self.document_store)\n",
    "\n",
    "        txt_pipeline.add_component(\"text_converter\", text_converter)\n",
    "        txt_pipeline.add_component(\"cleaner\", cleaner)\n",
    "        txt_pipeline.add_component(\"splitter\", splitter)\n",
    "        txt_pipeline.add_component(\"writer\", writer)\n",
    "\n",
    "        txt_pipeline.connect(\"text_converter\", \"cleaner\")\n",
    "        txt_pipeline.connect(\"cleaner\", \"splitter\")\n",
    "        txt_pipeline.connect(\"splitter\", \"writer\")\n",
    "        return txt_pipeline\n",
    "\n",
    "    def create_csv_pipeline(self, path):\n",
    "        \"\"\"Pipeline pour indexer les fichiers CSV\"\"\"\n",
    "        # Lecture du CSV\n",
    "        data = pd.read_csv(path)\n",
    "\n",
    "        # Création d'une liste de documents Haystack\n",
    "        documents = []\n",
    "        for index, row in data.iterrows():\n",
    "            content = ' '.join(f\"{col}: {val}\" for col, val in row.items())  # Conversion d'une ligne en texte\n",
    "            documents.append(Document(content=content, id=f\"csv_row_{index}\"))  # Ajout d'un ID unique\n",
    "\n",
    "        # Ajout des documents au Document Store avec gestion des doublons\n",
    "        self.document_store.write_documents(documents, policy=\"SKIP\")  # Ignorer les doublons\n",
    "\n",
    "    def run_index_pipeline(self, path):\n",
    "        file_extension = Path(path).suffix.lower()\n",
    "        if file_extension == \".pdf\":\n",
    "            pdf_pipeline = self.create_pdf_pipeline()\n",
    "            pdf_pipeline.run({\"pdf_converter\": {\"sources\": [Path(path)]}})\n",
    "        elif file_extension == \".txt\":\n",
    "            txt_pipeline = self.create_txt_pipeline()\n",
    "            txt_pipeline.run({\"text_converter\": {\"sources\": [Path(path)]}})\n",
    "        elif file_extension == \".csv\":\n",
    "            self.create_csv_pipeline(path)\n",
    "        else:\n",
    "            raise ValueError(\"Type de fichier non pris en charge. Veuillez utiliser un fichier PDF, TXT ou CSV.\")\n",
    "\n",
    "        # Vérification : Afficher les documents dans le document_store\n",
    "        all_docs = self.document_store.get_all_documents()\n",
    "        print(f\"Documents ajoutés au document_store : {len(all_docs)}\")\n",
    "        for doc in all_docs[:5]:  # Afficher les 5 premiers documents pour vérification\n",
    "            print(doc.content)\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, document_store, generator):\n",
    "        self.template = \"\"\"\n",
    "                        {% for document in documents %}\n",
    "                            {{ document.content }}\n",
    "                        {% endfor %}\n",
    "\n",
    "                        Please answer the question based on the given information.\n",
    "\n",
    "                        {{question}}\n",
    "                        \"\"\"\n",
    "        self.prompt_builder = PromptBuilder(template=self.template)\n",
    "        self.rag_pipeline = Pipeline()\n",
    "        self.retriever = InMemoryBM25Retriever(document_store)\n",
    "\n",
    "        self.rag_pipeline.add_component(\"retriever\", self.retriever)\n",
    "        self.rag_pipeline.add_component(\"prompt_builder\", self.prompt_builder)\n",
    "        self.rag_pipeline.add_component(\"llm\", generator)\n",
    "\n",
    "        self.rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "        self.rag_pipeline.connect(\"prompt_builder\", \"llm\")\n",
    "\n",
    "    def run_pipeline(self, query):\n",
    "        # Vérification : Exécuter une récupération de documents\n",
    "        docs = self.retriever.retrieve(query=query, top_k=5)\n",
    "        print(f\"Documents récupérés pour la question '{query}':\")\n",
    "        for doc in docs:\n",
    "            print(doc.content)\n",
    "\n",
    "        # Exécuter le pipeline RAG complet\n",
    "        res = self.rag_pipeline.run(\n",
    "            {\n",
    "                \"retriever\": {\"query\": query},\n",
    "                \"prompt_builder\": {\"question\": query},\n",
    "            }\n",
    "        )\n",
    "        return res\n",
    "\n",
    "# Initialisation du document store et des composants\n",
    "doc_store = InMemoryDocumentStore()\n",
    "idx = Indexing(doc_store)\n",
    "\n",
    "# Spécifiez ici le chemin du fichier que vous voulez indexer\n",
    "file_path = \"C:/Users/sarra/Desktop/iris.csv\"  # Remplacez par le chemin réel du fichier CSV\n",
    "idx.run_index_pipeline(file_path)\n",
    "\n",
    "# Initialisation du générateur\n",
    "generator = OpenAIGenerator(\n",
    "    api_key=Secret.from_env_var(\"GROQ_API_KEY\"),\n",
    "    api_base_url=\"https://api.groq.com/openai/v1\",\n",
    "    model=\"gemma2-9b-it\",\n",
    "    generation_kwargs={\"max_tokens\": 4096},\n",
    ")\n",
    "\n",
    "# Création de l'objet de requête\n",
    "query = Query(doc_store, generator)\n",
    "\n",
    "# Spécifiez ici la requête que vous souhaitez exécuter\n",
    "question = \"quel est le resumé de ce document?\"  # Remplacez par votre question\n",
    "\n",
    "# Exécution de la requête et affichage de la réponse\n",
    "response = query.run_pipeline(question)\n",
    "print(response[\"llm\"][\"replies\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92542a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
